{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f2BkIn0fngR"
      },
      "source": [
        "# LIDC IDRI 2D SEGMENTATION WITH TERNARY CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnmGk7-_fwzD",
        "outputId": "a562deb1-8871-4ec8-98c4-a0b60ab06d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd 'drive/Shareddrives/CLPT Thesis Bois/AISL-3-2021-C3/LIDC-2D-Segmentation-Colab'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyygLBwogtUm",
        "outputId": "b195c189-c8c3-4751-93d5-06324aa25a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/CLPT Thesis Bois/AISL-3-2021-C3/LIDC-2D-Segmentation-Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7B3yamNfngX"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2MJ-SVrLfngY"
      },
      "outputs": [],
      "source": [
        "# !pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfSdQhFEfnga"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import argparse\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from glob import glob\n",
        "import yaml\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision import transforms\n",
        "import torchsummary as summary\n",
        "from torchmetrics import Dice, JaccardIndex, ROC\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "from Unet_new.unet_model import UNet\n",
        "from UnetNested.Nested_Unet import NestedUNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyaykKN2fngc"
      },
      "source": [
        "## Define Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mue4WdgNfngc"
      },
      "outputs": [],
      "source": [
        "name = \"UNet\"           #default = \"UNet\"; can be NestedUNet\n",
        "epochs = 100            #default = 400\n",
        "batch_size = 4          #default = 12\n",
        "early_stopping = 50     #default = 50\n",
        "num_workers = 12        #default = 8\n",
        "optimizer = 'Adam'      #default = 'Adam'; can be SGD\n",
        "lr = 1e-5               #default = 1e-5\n",
        "momentum = 0.9          #default = 0.9\n",
        "weight_decay = 1e-4     #default = 1e-4\n",
        "nesterov = False        #default = False\n",
        "augmentation = True     #default = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5zDl0wYfngc"
      },
      "source": [
        "## Define Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEtRbzaAfngf"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r0UAim3fngf"
      },
      "outputs": [],
      "source": [
        "class LidcDataset(Dataset):\n",
        "    def __init__(self, IMAGES_PATHS, MASK_PATHS, transforms):\n",
        "        self.image_paths = IMAGES_PATHS\n",
        "        self.mask_paths = MASK_PATHS\n",
        "        \n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = np.load(self.image_paths[index])\n",
        "        mask = np.load(self.mask_paths[index])\n",
        "\n",
        "        #Make image and mask 3 dimensional\n",
        "        image = image.reshape(512,512,1)\n",
        "        mask = mask.reshape(512,512,1)\n",
        "\n",
        "        #Convert datatype\n",
        "        mask = mask.astype('uint8')\n",
        "\n",
        "        #Apply augmentation\n",
        "        augmented = self.transforms(image=image,mask=mask)\n",
        "        image = augmented['image']\n",
        "        mask = augmented['mask']\n",
        "        mask = mask.reshape([1,512,512])\n",
        "\n",
        "        image, mask = image.type(torch.FloatTensor), mask.type(torch.FloatTensor)     \n",
        "\n",
        "        return image, mask\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose([\n",
        "            A.ElasticTransform(alpha=1.1,alpha_affine=0.5,sigma=5,p=0.15),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            ToTensorV2()\n",
        "        ])"
      ],
      "metadata": {
        "id": "ei8vrk4Y3lek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOYDGIUYfngg"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mmIyJNAfngg"
      },
      "outputs": [],
      "source": [
        "# def iou_score_multiclass(output, target, n_classes):\n",
        "#     output = torch.nn.functional.softmax(output, dim=1)\n",
        "#     output = torch.argmax(output, dim=1).squeeze(1)\n",
        "#     target = torch.argmax(target, dim=1)\n",
        "#     iou_list = list()\n",
        "#     curr_iou_list = list()\n",
        "\n",
        "#     output = output.view(-1)\n",
        "#     target = target.view(-1)\n",
        "\n",
        "#     for sem_class in range(n_classes):\n",
        "#         output_inds = (output == sem_class)\n",
        "#         target_inds = (target == sem_class)\n",
        "\n",
        "#         if target_inds.long().sum().item() == 0:\n",
        "#             iou_curr = float('nan')\n",
        "#         else:\n",
        "#             intersection_curr = (output_inds[target_inds]).long().sum().item()\n",
        "#             union_curr = output_inds.long().sum().item() + target_inds.long().sum().item() - intersection_curr\n",
        "#             iou_curr = float(intersection_curr) / float(union_curr)\n",
        "#             curr_iou_list.append(iou_curr)\n",
        "#         iou_list.append(iou_curr)\n",
        "\n",
        "#     return np.mean(curr_iou_list)\n",
        "\n",
        "# # def dice_coef(output, target):\n",
        "# #     smooth = 1e-5\n",
        "# #     target_f = target.flatten()\n",
        "# #     output_f = output.flatten()\n",
        "# #     intersection = np.sum(target_f * output_f)\n",
        "# #     return (2. * intersection + smooth) / (np.sum(target_f) + np.sum(output_f) + smooth)\n",
        "\n",
        "# def dice_coef_multiclass(output, target, n_classes):\n",
        "#     smooth = 1e-5 \n",
        "#     output = torch.nn.functional.softmax(output, dim=1)\n",
        "#     output = torch.argmax(output, dim=1).squeeze(1)\n",
        "#     target = torch.argmax(target, dim=1)\n",
        "#     intersection = (output*target).sum()\n",
        "\n",
        "#     dice = (2. * intersection + smooth) / \\\n",
        "#         (output.sum() + target.sum() + smooth)\n",
        "#     return dice.item()\n",
        "\n",
        "#     # dice = 0\n",
        "#     # for i in range(n_classes):\n",
        "#     #     dice += dice_coef(output[:,i,:,:], target[:,:,:])\n",
        "#     # return dice/n_classes\n",
        "\n",
        "# def sensitivity_metric_multiclass(output, target):\n",
        "#     eps = 1e-5\n",
        "#     output = torch.nn.functional.softmax(output, dim=1)\n",
        "#     output = torch.argmax(output, dim=1).squeeze(1)\n",
        "#     target = torch.argmax(target, dim=1)\n",
        "#     # elements of confusion matrix\n",
        "#     tp = torch.sum(output * target) # True Positive\n",
        "#     fp = torch.sum(output * (1 - target)) # False Positive\n",
        "#     fn = torch.sum((1 - output) * target) # False Negative\n",
        "#     tn = torch.sum((1 - output) * (1 - target)) # True Negative\n",
        "#     # compute sensitivity\n",
        "#     sensitivity = (tp + eps) / (tp + fn + eps)\n",
        "    \n",
        "#     return sensitivity.item()\n",
        "\n",
        "\n",
        "# def dice_coef2(output, target):\n",
        "#     \"This metric is for validation\"\n",
        "#     smooth = 1e-5\n",
        "#     output = output.view(-1)\n",
        "#     output = (output>0.5).float().cpu().numpy()\n",
        "#     target = target.view(-1).data.cpu().numpy()\n",
        "#     intersection = (output*target).sum()\n",
        "\n",
        "#     return (2. * intersection + smooth) / \\\n",
        "#         (output.sum() + target.sum() + smooth)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def iou_score(output, target):\n",
        "#     smooth = 1e-5\n",
        "#     if torch.is_tensor(output):\n",
        "#         output = torch.sigmoid(output).data.cpu().numpy()\n",
        "#     if torch.is_tensor(target):\n",
        "#         target = target.data.cpu().numpy()\n",
        "#     output_ = output > 0.5\n",
        "#     target_ = target > 0.5\n",
        "#     intersection = (output_ & target_).sum()\n",
        "#     union = (output_ | target_).sum()\n",
        "\n",
        "#     return (intersection + smooth) / (union + smooth)\n",
        "\n",
        "\n",
        "# def dice_coef(output, target):\n",
        "#     smooth = 1e-5\n",
        "#     target_f = target.flatten()\n",
        "#     output_f = output.flatten()\n",
        "#     intersection = np.sum(target_f * output_f)\n",
        "#     return (2. * intersection + smooth) / (np.sum(target_f) + np.sum(output_f) + smooth)\n",
        "#     #Sigmoid is used because the U-Net output is logit\n",
        "#     # output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n",
        "#     # target = target.view(-1).data.cpu().numpy()\n",
        "#     # intersection = (output*target).sum()\n",
        "\n",
        "#     # return (2. * intersection + smooth) / \\\n",
        "#     #     (output.sum() + target.sum() + smooth)\n",
        "\n",
        "\n",
        "# def sensitivity_metric(output, target):\n",
        "#     eps = 1e-5\n",
        "#     output = torch.sigmoid(output).view(-1).data.cpu()\n",
        "#     target = target.view(-1).data.cpu()\n",
        "#     # elements of confusion matrix\n",
        "#     tp = torch.sum(output * target) # True Positive\n",
        "#     fp = torch.sum(output * (1 - target)) # False Positive\n",
        "#     fn = torch.sum((1 - output) * target) # False Negative\n",
        "#     tn = torch.sum((1 - output) * (1 - target)) # True Negative\n",
        "#     # compute sensitivity\n",
        "#     sensitivity = (tp + eps) / (tp + fn + eps)\n",
        "    \n",
        "#     return sensitivity.item()\n",
        "\n",
        "\n",
        "# def dice_coef2(output, target):\n",
        "#     \"This metric is for validation\"\n",
        "#     smooth = 1e-5\n",
        "#     output = output.view(-1)\n",
        "#     output = (output>0.5).float().cpu().numpy()\n",
        "#     target = target.view(-1).data.cpu().numpy()\n",
        "#     intersection = (output*target).sum()\n",
        "\n",
        "#     return (2. * intersection + smooth) / \\\n",
        "#         (output.sum() + target.sum() + smooth)"
      ],
      "metadata": {
        "id": "2eWnB-CoUhRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dice_coef = Dice(num_classes=4, multiclass=True).cuda() if torch.cuda.is_available() else Dice(num_classes=4, multiclass=True)\n",
        "iou_score = JaccardIndex(num_classes=4, multilabel=False).cuda() if torch.cuda.is_available() else JaccardIndex(num_classes=4, multilabel=False)\n",
        "roc = ROC(num_classes=4).cuda() if torch.cuda.is_available() else ROC(num_classes=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1csbMXl2WGB",
        "outputId": "053ebc82-8f86-46a8-909c-4d95166dab28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `ROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sensitivity_metric(output, target):\n",
        "  if target.dim() > 3:\n",
        "    target = torch.argmax(target, dim=1)\n",
        "  fpr, tpr, thresholds = roc(output, target)\n",
        "  return tpr"
      ],
      "metadata": {
        "id": "VJDQsVbk9GJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9KwWQWgfngh"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8sg63v7fngh"
      },
      "outputs": [],
      "source": [
        "def str_to_bool(v):\n",
        "    if v.lower() in ['true', 1]:\n",
        "        return True\n",
        "    elif v.lower() in ['false', 0]:\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "class AverageMeter(object):\n",
        "    #Computes and stores the average and current value\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "    \n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn2h7c3Ffngh"
      },
      "source": [
        "## Get Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtuiGqoGfngh",
        "outputId": "ad096009-1e8b-44ed-d5ac-c5cd17095f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating directory called  UNet_with_augmentation\n",
            "--------------------\n",
            "Configuration Setting: \n",
            "Model:  UNet\n",
            "Max Epochs:  100\n",
            "Batch Size:  4\n",
            "Number of Workers:  12\n",
            "Optimizer:  Adam\n",
            "Learning Rate:  1e-05\n",
            "Augmentation:  True\n"
          ]
        }
      ],
      "source": [
        "if augmentation == True:\n",
        "    file_name = name + '_with_augmentation'\n",
        "else:\n",
        "    file_name = name + '_base'\n",
        "os.makedirs('model_outputs/{}'.format(file_name), exist_ok=True)\n",
        "print(\"Creating directory called \", file_name)\n",
        "\n",
        "print('-' * 20)\n",
        "print(\"Configuration Setting: \")\n",
        "print(\"Model: \", name)\n",
        "print(\"Max Epochs: \", epochs)\n",
        "print(\"Batch Size: \", batch_size)\n",
        "print(\"Number of Workers: \", num_workers)\n",
        "print(\"Optimizer: \", optimizer)\n",
        "print(\"Learning Rate: \", lr)\n",
        "print(\"Augmentation: \", augmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7n4I2d8fngi"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuxjmCnzfngi",
        "outputId": "259571ea-9cba-42f7-f442-d150049be465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model...\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 512, 512]             640\n",
            "       BatchNorm2d-2         [-1, 64, 512, 512]             128\n",
            "              ReLU-3         [-1, 64, 512, 512]               0\n",
            "            Conv2d-4         [-1, 64, 512, 512]          36,928\n",
            "       BatchNorm2d-5         [-1, 64, 512, 512]             128\n",
            "              ReLU-6         [-1, 64, 512, 512]               0\n",
            "       double_conv-7         [-1, 64, 512, 512]               0\n",
            "            inconv-8         [-1, 64, 512, 512]               0\n",
            "         MaxPool2d-9         [-1, 64, 256, 256]               0\n",
            "           Conv2d-10        [-1, 128, 256, 256]          73,856\n",
            "      BatchNorm2d-11        [-1, 128, 256, 256]             256\n",
            "             ReLU-12        [-1, 128, 256, 256]               0\n",
            "           Conv2d-13        [-1, 128, 256, 256]         147,584\n",
            "      BatchNorm2d-14        [-1, 128, 256, 256]             256\n",
            "             ReLU-15        [-1, 128, 256, 256]               0\n",
            "      double_conv-16        [-1, 128, 256, 256]               0\n",
            "             down-17        [-1, 128, 256, 256]               0\n",
            "        MaxPool2d-18        [-1, 128, 128, 128]               0\n",
            "           Conv2d-19        [-1, 256, 128, 128]         295,168\n",
            "      BatchNorm2d-20        [-1, 256, 128, 128]             512\n",
            "             ReLU-21        [-1, 256, 128, 128]               0\n",
            "           Conv2d-22        [-1, 256, 128, 128]         590,080\n",
            "      BatchNorm2d-23        [-1, 256, 128, 128]             512\n",
            "             ReLU-24        [-1, 256, 128, 128]               0\n",
            "      double_conv-25        [-1, 256, 128, 128]               0\n",
            "             down-26        [-1, 256, 128, 128]               0\n",
            "        MaxPool2d-27          [-1, 256, 64, 64]               0\n",
            "           Conv2d-28          [-1, 512, 64, 64]       1,180,160\n",
            "      BatchNorm2d-29          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-30          [-1, 512, 64, 64]               0\n",
            "           Conv2d-31          [-1, 512, 64, 64]       2,359,808\n",
            "      BatchNorm2d-32          [-1, 512, 64, 64]           1,024\n",
            "             ReLU-33          [-1, 512, 64, 64]               0\n",
            "      double_conv-34          [-1, 512, 64, 64]               0\n",
            "             down-35          [-1, 512, 64, 64]               0\n",
            "        MaxPool2d-36          [-1, 512, 32, 32]               0\n",
            "           Conv2d-37          [-1, 512, 32, 32]       2,359,808\n",
            "      BatchNorm2d-38          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-39          [-1, 512, 32, 32]               0\n",
            "           Conv2d-40          [-1, 512, 32, 32]       2,359,808\n",
            "      BatchNorm2d-41          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-42          [-1, 512, 32, 32]               0\n",
            "      double_conv-43          [-1, 512, 32, 32]               0\n",
            "             down-44          [-1, 512, 32, 32]               0\n",
            "         Upsample-45          [-1, 512, 64, 64]               0\n",
            "           Conv2d-46          [-1, 256, 64, 64]       2,359,552\n",
            "      BatchNorm2d-47          [-1, 256, 64, 64]             512\n",
            "             ReLU-48          [-1, 256, 64, 64]               0\n",
            "           Conv2d-49          [-1, 256, 64, 64]         590,080\n",
            "      BatchNorm2d-50          [-1, 256, 64, 64]             512\n",
            "             ReLU-51          [-1, 256, 64, 64]               0\n",
            "      double_conv-52          [-1, 256, 64, 64]               0\n",
            "               up-53          [-1, 256, 64, 64]               0\n",
            "         Upsample-54        [-1, 256, 128, 128]               0\n",
            "           Conv2d-55        [-1, 128, 128, 128]         589,952\n",
            "      BatchNorm2d-56        [-1, 128, 128, 128]             256\n",
            "             ReLU-57        [-1, 128, 128, 128]               0\n",
            "           Conv2d-58        [-1, 128, 128, 128]         147,584\n",
            "      BatchNorm2d-59        [-1, 128, 128, 128]             256\n",
            "             ReLU-60        [-1, 128, 128, 128]               0\n",
            "      double_conv-61        [-1, 128, 128, 128]               0\n",
            "               up-62        [-1, 128, 128, 128]               0\n",
            "         Upsample-63        [-1, 128, 256, 256]               0\n",
            "           Conv2d-64         [-1, 64, 256, 256]         147,520\n",
            "      BatchNorm2d-65         [-1, 64, 256, 256]             128\n",
            "             ReLU-66         [-1, 64, 256, 256]               0\n",
            "           Conv2d-67         [-1, 64, 256, 256]          36,928\n",
            "      BatchNorm2d-68         [-1, 64, 256, 256]             128\n",
            "             ReLU-69         [-1, 64, 256, 256]               0\n",
            "      double_conv-70         [-1, 64, 256, 256]               0\n",
            "               up-71         [-1, 64, 256, 256]               0\n",
            "         Upsample-72         [-1, 64, 512, 512]               0\n",
            "           Conv2d-73         [-1, 64, 512, 512]          73,792\n",
            "      BatchNorm2d-74         [-1, 64, 512, 512]             128\n",
            "             ReLU-75         [-1, 64, 512, 512]               0\n",
            "           Conv2d-76         [-1, 64, 512, 512]          36,928\n",
            "      BatchNorm2d-77         [-1, 64, 512, 512]             128\n",
            "             ReLU-78         [-1, 64, 512, 512]               0\n",
            "      double_conv-79         [-1, 64, 512, 512]               0\n",
            "               up-80         [-1, 64, 512, 512]               0\n",
            "           Conv2d-81          [-1, 4, 512, 512]             260\n",
            "          outconv-82          [-1, 4, 512, 512]               0\n",
            "================================================================\n",
            "Total params: 13,394,372\n",
            "Trainable params: 13,394,372\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.00\n",
            "Forward/backward pass size (MB): 3740.00\n",
            "Params size (MB): 51.10\n",
            "Estimated Total Size (MB): 3792.10\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        }
      ],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else torch.nn.CrossEntropyLoss()\n",
        "cudnn.benchmark = True\n",
        "\n",
        "#Creating the model\n",
        "print(\"Creating model...\")\n",
        "if name == 'NestedUNet':\n",
        "    model = NestedUNet(num_classes=4)\n",
        "else:\n",
        "    model = UNet(n_channels=1, n_classes=4)\n",
        "model = model.cuda() if torch.cuda.is_available() else model\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"We can use \", torch.cuda.device_count(), \" GPUs.\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "params = filter(lambda p: p.requires_grad, model.parameters())\n",
        "\n",
        "if optimizer == 'Adam':\n",
        "    optimizer = optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
        "elif optimizer == 'SGD':\n",
        "    optimizer = optim.SGD(params, lr=lr, momentum=momentum, nesterov=nesterov, weight_decay=weight_decay)\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "    \n",
        "summary.summary(model,(1,512,512))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxuoE_jLfngi"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k5hV9l5fngj",
        "outputId": "48c2794b-61b5-4d12-dc18-d646bc02991a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Original images: 980, masks: 980 for training.\n",
            "Original images: 177, masks: 177 for validation.\n",
            "Ratio between Validation and Training is 0.180612\n",
            "**************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "#directory of Images and Masks folders (generated from preprocessing)                                         \n",
        "IMAGE_DIR = 'LIDC-IDRI Preprocessed Exp 3/Image/'\n",
        "MASK_DIR = 'LIDC-IDRI Preprocessed Exp 3/Mask/'                                                                 \n",
        "\n",
        "#meta information\n",
        "meta = pd.read_csv('LIDC-IDRI Preprocessed Exp 3/Meta/meta.csv')\n",
        "meta = meta[meta['patient_diagnosis'] != 0]\n",
        "\n",
        "#Get train/test label from metadata file\n",
        "meta['original_image'] = meta['original_image'].apply(lambda x: IMAGE_DIR + \"LIDC-IDRI-\" + x[:4] + \"/\" + x + \".npy\")\n",
        "meta['mask_image'] = meta['mask_image'].apply(lambda x: MASK_DIR + \"LIDC-IDRI-\" + x[:4] + \"/\" + x + \".npy\")\n",
        "\n",
        "\n",
        "#Split into training and validation\n",
        "train_meta = meta[meta['data_split']=='Train']\n",
        "val_meta = meta[meta['data_split']=='Validation']\n",
        "\n",
        "#Get training images into list\n",
        "train_image_paths = list(train_meta['original_image'])\n",
        "train_mask_paths = list(train_meta['mask_image'])\n",
        "\n",
        "#Get validation images into list\n",
        "val_image_paths = list(val_meta['original_image'])\n",
        "val_mask_paths = list(val_meta['mask_image'])\n",
        "\n",
        "print(\"*\"*50)\n",
        "print(\"Original images: {}, masks: {} for training.\".format(len(train_image_paths),len(train_mask_paths)))\n",
        "print(\"Original images: {}, masks: {} for validation.\".format(len(val_image_paths),len(val_mask_paths)))\n",
        "print(\"Ratio between Validation and Training is {:2f}\".format(len(val_image_paths)/len(train_image_paths)))\n",
        "print(\"*\"*50)\n",
        "\n",
        "\n",
        "#Creating custom LIDC dataset\n",
        "train_dataset = LidcDataset(train_image_paths, train_mask_paths, transforms=transform)\n",
        "val_dataset = LidcDataset(val_image_paths, val_mask_paths, transforms=transform)\n",
        "\n",
        "#Creating Dataloader\n",
        "train_loader = DataLoader(\n",
        "  train_dataset,\n",
        "  batch_size=batch_size,\n",
        "  shuffle=True,\n",
        "  pin_memory=True,\n",
        "  drop_last=True,\n",
        "  num_workers=num_workers\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "  val_dataset,\n",
        "  batch_size=batch_size,\n",
        "  shuffle=False,\n",
        "  pin_memory=True,\n",
        "  drop_last=False,\n",
        "  num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFUeTMtlfngj"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FMvFNYA-f6V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkIDHbbffngk",
        "outputId": "52534728-e28a-4135-b5fc-ec7a33c38d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [22:00<00:00,  5.39s/it, loss=1.26, iou=0.215, dice=0.86]\n",
            "100%|██████████| 45/45 [04:02<00:00,  5.39s/it, val_loss=1.21, val_iou=0.233, val_dice=0.932]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 1/100,  Training Loss: 1.2578,  Training DICE: 0.8598,  Training IOU: 0.2149,  Validation Loss: 1.2084,  Validation DICE: 0.9316,  Validation IOU: 0.2329\n",
            "Saved new best model based on DICE metric!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [23:52<00:00,  5.85s/it, loss=1.19, iou=0.233, dice=0.931]\n",
            "100%|██████████| 45/45 [04:03<00:00,  5.40s/it, val_loss=1.17, val_iou=0.236, val_dice=0.944]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 2/100,  Training Loss: 1.1896,  Training DICE: 0.9310,  Training IOU: 0.2327,  Validation Loss: 1.1711,  Validation DICE: 0.9436,  Validation IOU: 0.2359\n",
            "Saved new best model based on DICE metric!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [24:03<00:00,  5.89s/it, loss=1.16, iou=0.234, dice=0.937]\n",
            "100%|██████████| 45/45 [04:04<00:00,  5.44s/it, val_loss=1.15, val_iou=0.237, val_dice=0.949]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 3/100,  Training Loss: 1.1646,  Training DICE: 0.9375,  Training IOU: 0.2344,  Validation Loss: 1.1529,  Validation DICE: 0.9487,  Validation IOU: 0.2372\n",
            "Saved new best model based on DICE metric!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [24:11<00:00,  5.93s/it, loss=1.15, iou=0.236, dice=0.942]\n",
            "100%|██████████| 45/45 [04:07<00:00,  5.50s/it, val_loss=1.14, val_iou=0.238, val_dice=0.952]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 4/100,  Training Loss: 1.1470,  Training DICE: 0.9421,  Training IOU: 0.2355,  Validation Loss: 1.1371,  Validation DICE: 0.9519,  Validation IOU: 0.2380\n",
            "Saved new best model based on DICE metric!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [24:22<00:00,  5.97s/it, loss=1.13, iou=0.237, dice=0.947]\n",
            "100%|██████████| 45/45 [04:09<00:00,  5.54s/it, val_loss=1.12, val_iou=0.239, val_dice=0.957]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 5/100,  Training Loss: 1.1308,  Training DICE: 0.9472,  Training IOU: 0.2368,  Validation Loss: 1.1208,  Validation DICE: 0.9569,  Validation IOU: 0.2392\n",
            "Saved new best model based on DICE metric!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [24:31<00:00,  6.01s/it, loss=1.11, iou=0.238, dice=0.953]\n",
            "100%|██████████| 45/45 [04:09<00:00,  5.54s/it, val_loss=1.11, val_iou=0.24, val_dice=0.961]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 6/100,  Training Loss: 1.1146,  Training DICE: 0.9534,  Training IOU: 0.2384,  Validation Loss: 1.1084,  Validation DICE: 0.9613,  Validation IOU: 0.2403\n",
            "Saved new best model based on DICE metric!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 245/245 [24:44<00:00,  6.06s/it, loss=1.1, iou=0.24, dice=0.96]\n",
            "100%|██████████| 45/45 [04:12<00:00,  5.62s/it, val_loss=1.09, val_iou=0.242, val_dice=0.967]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 7/100,  Training Loss: 1.0997,  Training DICE: 0.9597,  Training IOU: 0.2399,  Validation Loss: 1.0914,  Validation DICE: 0.9671,  Validation IOU: 0.2418\n",
            "Saved new best model based on DICE metric!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 86/245 [08:46<16:18,  6.15s/it, loss=1.09, iou=0.241, dice=0.965]"
          ]
        }
      ],
      "source": [
        "# log = pd.DataFrame(index=[], columns=['epoch','lr','loss','iou','dice','sensitivity','val_loss','val_iou'])\n",
        "log = pd.DataFrame(index=[], columns=['epoch','lr','loss','iou','dice','val_loss','val_iou'])\n",
        "\n",
        "best_dice = 0\n",
        "trigger = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    #Model Training\n",
        "    # avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter(), 'dice': AverageMeter(), 'sensitivity': AverageMeter()}\n",
        "    avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter(), 'dice': AverageMeter()}\n",
        "    model.train()\n",
        "    pbar = tqdm(total=len(train_loader)) #progress bar\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        input = data[0].cuda()\n",
        "        target = data[1].cuda()\n",
        "        output = model(input)\n",
        "\n",
        "        #Get loss and metric\n",
        "        loss = criterion(output, torch.argmax(target, dim=1))\n",
        "        iou = iou_score(output, torch.argmax(target, dim=1))\n",
        "        dice = dice_coef(output, torch.argmax(target, dim=1))\n",
        "        # sensitivity = sensitivity_metric(output, torch.argmax(target, dim=1))\n",
        "\n",
        "        #Calculate the gradient and perform optimizing step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #Update average metrics\n",
        "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
        "        avg_meters['iou'].update(iou.item(), input.size(0))\n",
        "        avg_meters['dice'].update(dice.item(), input.size(0))\n",
        "        # avg_meters['sensitivity'].update(sensitivity, input.size(0))\n",
        "\n",
        "        postfix = OrderedDict([\n",
        "            ('loss', avg_meters['loss'].avg),\n",
        "            ('iou', avg_meters['iou'].avg),\n",
        "            ('dice', avg_meters['dice'].avg),\n",
        "            # ('sensitivity', avg_meters['sensitivity'].avg)\n",
        "        ])\n",
        "        pbar.set_postfix(postfix)\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "\n",
        "    train_log = OrderedDict([\n",
        "        ('loss', avg_meters['loss'].avg),\n",
        "        ('iou', avg_meters['iou'].avg),\n",
        "        ('dice', avg_meters['dice'].avg),\n",
        "        # ('sensitivity', avg_meters['sensitivity'].avg)\n",
        "    ])\n",
        "\n",
        "\n",
        "    #Model Validation\n",
        "    # val_avg_meters = {'val_loss': AverageMeter(), 'val_iou': AverageMeter(), 'val_dice': AverageMeter(), 'val_sensitivity': AverageMeter()}\n",
        "    val_avg_meters = {'val_loss': AverageMeter(), 'val_iou': AverageMeter(), 'val_dice': AverageMeter()}\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_pbar = tqdm(total=len(val_loader))\n",
        "        for i, val_data in enumerate(val_loader):\n",
        "\n",
        "            val_input = val_data[0].cuda()\n",
        "            val_target = val_data[1].cuda()\n",
        "            val_output = model(val_input)\n",
        "\n",
        "            val_loss = criterion(val_output, torch.argmax(val_target, dim=1))\n",
        "            val_iou = iou_score(val_output, torch.argmax(val_target, dim=1))\n",
        "            val_dice = dice_coef(val_output, torch.argmax(val_target, dim=1))\n",
        "            # val_sensitivity = sensitivity_metric(val_output, torch.argmax(val_target, dim=1))\n",
        "\n",
        "            val_avg_meters['val_loss'].update(val_loss.item(), val_input.size(0))\n",
        "            val_avg_meters['val_iou'].update(val_iou.item(), val_input.size(0))\n",
        "            val_avg_meters['val_dice'].update(val_dice.item(), val_input.size(0))\n",
        "            # val_avg_meters['val_sensitivity'].update(val_sensitivity, val_input.size(0))\n",
        "\n",
        "            val_postfix = OrderedDict([\n",
        "                ('val_loss', val_avg_meters['val_loss'].avg),\n",
        "                ('val_iou', val_avg_meters['val_iou'].avg),\n",
        "                ('val_dice', val_avg_meters['val_dice'].avg),\n",
        "                # ('val_sensitivity', val_avg_meters['val_sensitivity'].avg)\n",
        "            ])\n",
        "            val_pbar.set_postfix(val_postfix)\n",
        "            val_pbar.update(1)\n",
        "        val_pbar.close()\n",
        "\n",
        "    val_log = OrderedDict([\n",
        "        ('val_loss', val_avg_meters['val_loss'].avg),\n",
        "        ('val_iou', val_avg_meters['val_iou'].avg),\n",
        "        ('val_dice', val_avg_meters['val_dice'].avg),\n",
        "        # ('val_sensitivity', val_avg_meters['val_sensitivity'].avg)\n",
        "    ])\n",
        "    \n",
        "\n",
        "    # print('Training Epoch {}/{},  Training Loss: {:.4f},  Training DICE: {:.4f},  Training IOU: {:.4f},  Training Sensitivity: {:.4f},  Validation Loss: {:.4f},  Validation DICE: {:.4f},  Validation IOU: {:.4f},  Validation Sensitivity: {:.4f}'.format(\n",
        "    #     epoch+1, epochs, train_log['loss'], train_log['dice'], train_log['iou'], train_log['sensitivity'], val_log['val_loss'], val_log['val_dice'], val_log['val_iou'], val_log['val_sensitivity']\n",
        "    # ))\n",
        "    print('Training Epoch {}/{},  Training Loss: {:.4f},  Training DICE: {:.4f},  Training IOU: {:.4f},  Validation Loss: {:.4f},  Validation DICE: {:.4f},  Validation IOU: {:.4f}'.format(\n",
        "        epoch+1, epochs, train_log['loss'], train_log['dice'], train_log['iou'], val_log['val_loss'], val_log['val_dice'], val_log['val_iou']\n",
        "    ))\n",
        "\n",
        "    #Save values to csv file\n",
        "    tmp = pd.Series([\n",
        "        epoch,\n",
        "        lr,\n",
        "        train_log['loss'],\n",
        "        train_log['iou'],\n",
        "        train_log['dice'],\n",
        "        # train_log['sensitivity'],\n",
        "        val_log['val_loss'],\n",
        "        val_log['val_iou'],\n",
        "        val_log['val_dice'],\n",
        "        # val_log['val_sensitivity']\n",
        "    ], index=['epoch', 'lr', 'loss', 'iou', 'dice', 'val_loss', 'val_iou', 'val_dice'])\n",
        "    # index=['epoch', 'lr', 'loss', 'iou', 'dice', 'val_loss', 'val_iou', 'val_dice', 'val_sensitivity'])\n",
        "\n",
        "    log = log.append(tmp, ignore_index=True)\n",
        "    log.to_csv('model_outputs/{}/log.csv'.format(file_name), index=False)\n",
        "\n",
        "    trigger += 1\n",
        "\n",
        "    #If best DICE score, save the model\n",
        "    if val_log['val_dice'] > best_dice:\n",
        "        torch.save(model.state_dict(), 'model_outputs/{}/model.pth'.format(file_name))\n",
        "        best_dice = val_log['val_dice']\n",
        "        print(\"Saved new best model based on DICE metric!\")\n",
        "        trigger = 0\n",
        "    \n",
        "    if early_stopping >= 0 and trigger >= early_stopping:\n",
        "        print(\"Early stopping.\")\n",
        "        break\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fPuk2P8ixSn1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "cb56554e22e7015c8034fdf988622508a740e712e9b2a73e8d246c741229446d"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}