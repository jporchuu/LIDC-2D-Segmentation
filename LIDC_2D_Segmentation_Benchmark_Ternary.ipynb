{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f2BkIn0fngR"
   },
   "source": [
    "# LIDC IDRI 2D SEGMENTATION WITH TERNARY CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7B3yamNfngX"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1665649177992,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "2MJ-SVrLfngY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1665649180189,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "mfSdQhFEfnga"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "import torchsummary as summary\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Unet_new.unet_model import UNet\n",
    "from UnetNested.Nested_Unet import NestedUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyaykKN2fngc"
   },
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1665649180190,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "Mue4WdgNfngc"
   },
   "outputs": [],
   "source": [
    "name = \"UNet\"           #default = \"UNet\"; can be NestedUNet\n",
    "epochs = 100            #default = 400\n",
    "batch_size = 12         #default = 12\n",
    "early_stopping = 50     #default = 50\n",
    "num_workers = 8         #default = 8\n",
    "optimizer = 'Adam'      #default = 'Adam'; can be SGD\n",
    "lr = 1e-5               #default = 1e-5\n",
    "momentum = 0.9          #default = 0.9\n",
    "weight_decay = 1e-4     #default = 1e-4\n",
    "nesterov = False        #default = False\n",
    "augmentation = True     #default = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5zDl0wYfngc"
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEtRbzaAfngf"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1665649180191,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "1r0UAim3fngf"
   },
   "outputs": [],
   "source": [
    "class LidcDataset(Dataset):\n",
    "    def __init__(self, IMAGES_PATHS, MASK_PATHS, transforms):\n",
    "        self.image_paths = IMAGES_PATHS\n",
    "        self.mask_paths = MASK_PATHS\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = np.load(self.image_paths[index])\n",
    "        mask = np.load(self.mask_paths[index])\n",
    "\n",
    "        #Make image and mask 3 dimensional\n",
    "        image = image.reshape(512,512,1)\n",
    "        mask = mask.reshape(512,512,1)\n",
    "\n",
    "        #Convert datatype\n",
    "        mask = mask.astype('uint8')\n",
    "\n",
    "        #Apply augmentation\n",
    "        augmented = self.transforms(image=image,mask=mask)\n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask']\n",
    "        mask = mask.reshape([1,512,512])\n",
    "\n",
    "        image, mask = image.type(torch.FloatTensor), mask.type(torch.FloatTensor)     \n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1665649180191,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "ei8vrk4Y3lek"
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "            A.ElasticTransform(alpha=1.1,alpha_affine=0.5,sigma=5,p=0.15),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOYDGIUYfngg"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1665649180192,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "1mmIyJNAfngg"
   },
   "outputs": [],
   "source": [
    "def iou_score_multiclass(output, target, n_classes):\n",
    "    output = torch.nn.functional.softmax(output, dim=1)\n",
    "    output = torch.argmax(output, dim=1).squeeze(1)\n",
    "    target = torch.argmax(target, dim=1)\n",
    "    iou_list = list()\n",
    "    curr_iou_list = list()\n",
    "\n",
    "    output = output.view(-1)\n",
    "    target = target.view(-1)\n",
    "\n",
    "    for sem_class in range(n_classes):\n",
    "        output_inds = (output == sem_class)\n",
    "        target_inds = (target == sem_class)\n",
    "\n",
    "        if target_inds.long().sum().item() == 0:\n",
    "            iou_curr = float('nan')\n",
    "        else:\n",
    "            intersection_curr = (output_inds[target_inds]).long().sum().item()\n",
    "            union_curr = output_inds.long().sum().item() + target_inds.long().sum().item() - intersection_curr\n",
    "            iou_curr = float(intersection_curr) / float(union_curr)\n",
    "            curr_iou_list.append(iou_curr)\n",
    "        iou_list.append(iou_curr)\n",
    "\n",
    "    return np.mean(curr_iou_list)\n",
    "\n",
    "# def dice_coef(output, target):\n",
    "#     smooth = 1e-5\n",
    "#     target_f = target.flatten()\n",
    "#     output_f = output.flatten()\n",
    "#     intersection = np.sum(target_f * output_f)\n",
    "#     return (2. * intersection + smooth) / (np.sum(target_f) + np.sum(output_f) + smooth)\n",
    "\n",
    "def dice_coef_multiclass(output, target, n_classes):\n",
    "    smooth = 1e-5 \n",
    "    output = torch.nn.functional.softmax(output, dim=1)\n",
    "    output = torch.argmax(output, dim=1).squeeze(1)\n",
    "    target = torch.argmax(target, dim=1)\n",
    "    intersection = (output*target).sum()\n",
    "\n",
    "    dice = (2. * intersection + smooth) / \\\n",
    "        (output.sum() + target.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "    # dice = 0\n",
    "    # for i in range(n_classes):\n",
    "    #     dice += dice_coef(output[:,i,:,:], target[:,:,:])\n",
    "    # return dice/n_classes\n",
    "\n",
    "def sensitivity_metric_multiclass(output, target):\n",
    "    eps = 1e-5\n",
    "    output = torch.nn.functional.softmax(output, dim=1)\n",
    "    output = torch.argmax(output, dim=1).squeeze(1)\n",
    "    target = torch.argmax(target, dim=1)\n",
    "    # elements of confusion matrix\n",
    "    tp = torch.sum(output * target) # True Positive\n",
    "    fp = torch.sum(output * (1 - target)) # False Positive\n",
    "    fn = torch.sum((1 - output) * target) # False Negative\n",
    "    tn = torch.sum((1 - output) * (1 - target)) # True Negative\n",
    "    # compute sensitivity\n",
    "    sensitivity = (tp + eps) / (tp + fn + eps)\n",
    "    \n",
    "    return sensitivity.item()\n",
    "\n",
    "\n",
    "def dice_coef2(output, target):\n",
    "    \"This metric is for validation\"\n",
    "    smooth = 1e-5\n",
    "    output = output.view(-1)\n",
    "    output = (output>0.5).float().cpu().numpy()\n",
    "    target = target.view(-1).data.cpu().numpy()\n",
    "    intersection = (output*target).sum()\n",
    "\n",
    "    return (2. * intersection + smooth) / \\\n",
    "        (output.sum() + target.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1665649180192,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "2eWnB-CoUhRz"
   },
   "outputs": [],
   "source": [
    "# def iou_score(output, target):\n",
    "#     smooth = 1e-5\n",
    "#     if torch.is_tensor(output):\n",
    "#         output = torch.sigmoid(output).data.cpu().numpy()\n",
    "#     if torch.is_tensor(target):\n",
    "#         target = target.data.cpu().numpy()\n",
    "#     output_ = output > 0.5\n",
    "#     target_ = target > 0.5\n",
    "#     intersection = (output_ & target_).sum()\n",
    "#     union = (output_ | target_).sum()\n",
    "\n",
    "#     return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "\n",
    "# def dice_coef(output, target):\n",
    "#     smooth = 1e-5\n",
    "#     target_f = target.flatten()\n",
    "#     output_f = output.flatten()\n",
    "#     intersection = np.sum(target_f * output_f)\n",
    "#     return (2. * intersection + smooth) / (np.sum(target_f) + np.sum(output_f) + smooth)\n",
    "#     #Sigmoid is used because the U-Net output is logit\n",
    "#     # output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n",
    "#     # target = target.view(-1).data.cpu().numpy()\n",
    "#     # intersection = (output*target).sum()\n",
    "\n",
    "#     # return (2. * intersection + smooth) / \\\n",
    "#     #     (output.sum() + target.sum() + smooth)\n",
    "\n",
    "\n",
    "# def sensitivity_metric(output, target):\n",
    "#     eps = 1e-5\n",
    "#     output = torch.sigmoid(output).view(-1).data.cpu()\n",
    "#     target = target.view(-1).data.cpu()\n",
    "#     # elements of confusion matrix\n",
    "#     tp = torch.sum(output * target) # True Positive\n",
    "#     fp = torch.sum(output * (1 - target)) # False Positive\n",
    "#     fn = torch.sum((1 - output) * target) # False Negative\n",
    "#     tn = torch.sum((1 - output) * (1 - target)) # True Negative\n",
    "#     # compute sensitivity\n",
    "#     sensitivity = (tp + eps) / (tp + fn + eps)\n",
    "    \n",
    "#     return sensitivity.item()\n",
    "\n",
    "\n",
    "# def dice_coef2(output, target):\n",
    "#     \"This metric is for validation\"\n",
    "#     smooth = 1e-5\n",
    "#     output = output.view(-1)\n",
    "#     output = (output>0.5).float().cpu().numpy()\n",
    "#     target = target.view(-1).data.cpu().numpy()\n",
    "#     intersection = (output*target).sum()\n",
    "\n",
    "#     return (2. * intersection + smooth) / \\\n",
    "#         (output.sum() + target.sum() + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9KwWQWgfngh"
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1665649180192,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "V8sg63v7fngh"
   },
   "outputs": [],
   "source": [
    "def str_to_bool(v):\n",
    "    if v.lower() in ['true', 1]:\n",
    "        return True\n",
    "    elif v.lower() in ['false', 0]:\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    #Computes and stores the average and current value\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn2h7c3Ffngh"
   },
   "source": [
    "## Get Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1665649180193,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "rtuiGqoGfngh",
    "outputId": "d01cc4d5-888a-498e-fa0c-e9ac759e0b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory called  UNet_with_augmentation\n",
      "--------------------\n",
      "Configuration Setting: \n",
      "Model:  UNet\n",
      "Max Epochs:  100\n",
      "Batch Size:  12\n",
      "Number of Workers:  8\n",
      "Optimizer:  Adam\n",
      "Learning Rate:  1e-05\n",
      "Augmentation:  True\n"
     ]
    }
   ],
   "source": [
    "if augmentation == True:\n",
    "    file_name = name + '_with_augmentation'\n",
    "else:\n",
    "    file_name = name + '_base'\n",
    "os.makedirs('model_outputs/{}'.format(file_name), exist_ok=True)\n",
    "print(\"Creating directory called \", file_name)\n",
    "\n",
    "print('-' * 20)\n",
    "print(\"Configuration Setting: \")\n",
    "print(\"Model: \", name)\n",
    "print(\"Max Epochs: \", epochs)\n",
    "print(\"Batch Size: \", batch_size)\n",
    "print(\"Number of Workers: \", num_workers)\n",
    "print(\"Optimizer: \", optimizer)\n",
    "print(\"Learning Rate: \", lr)\n",
    "print(\"Augmentation: \", augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7n4I2d8fngi"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7780,
     "status": "ok",
     "timestamp": 1665649187964,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "QuxjmCnzfngi",
    "outputId": "0b2d0898-7ab6-4ab0-c083-4a736c631032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─inconv: 1-1                            [-1, 64, 512, 512]        --\n",
      "|    └─double_conv: 2-1                  [-1, 64, 512, 512]        --\n",
      "|    |    └─Sequential: 3-1              [-1, 64, 512, 512]        37,824\n",
      "├─down: 1-2                              [-1, 128, 256, 256]       --\n",
      "|    └─Sequential: 2-2                   [-1, 128, 256, 256]       --\n",
      "|    |    └─MaxPool2d: 3-2               [-1, 64, 256, 256]        --\n",
      "|    |    └─double_conv: 3-3             [-1, 128, 256, 256]       221,952\n",
      "├─down: 1-3                              [-1, 256, 128, 128]       --\n",
      "|    └─Sequential: 2-3                   [-1, 256, 128, 128]       --\n",
      "|    |    └─MaxPool2d: 3-4               [-1, 128, 128, 128]       --\n",
      "|    |    └─double_conv: 3-5             [-1, 256, 128, 128]       886,272\n",
      "├─down: 1-4                              [-1, 512, 64, 64]         --\n",
      "|    └─Sequential: 2-4                   [-1, 512, 64, 64]         --\n",
      "|    |    └─MaxPool2d: 3-6               [-1, 256, 64, 64]         --\n",
      "|    |    └─double_conv: 3-7             [-1, 512, 64, 64]         3,542,016\n",
      "├─down: 1-5                              [-1, 512, 32, 32]         --\n",
      "|    └─Sequential: 2-5                   [-1, 512, 32, 32]         --\n",
      "|    |    └─MaxPool2d: 3-8               [-1, 512, 32, 32]         --\n",
      "|    |    └─double_conv: 3-9             [-1, 512, 32, 32]         4,721,664\n",
      "├─up: 1-6                                [-1, 256, 64, 64]         --\n",
      "|    └─Upsample: 2-6                     [-1, 512, 64, 64]         --\n",
      "|    └─double_conv: 2-7                  [-1, 256, 64, 64]         --\n",
      "|    |    └─Sequential: 3-10             [-1, 256, 64, 64]         2,950,656\n",
      "├─up: 1-7                                [-1, 128, 128, 128]       --\n",
      "|    └─Upsample: 2-8                     [-1, 256, 128, 128]       --\n",
      "|    └─double_conv: 2-9                  [-1, 128, 128, 128]       --\n",
      "|    |    └─Sequential: 3-11             [-1, 128, 128, 128]       738,048\n",
      "├─up: 1-8                                [-1, 64, 256, 256]        --\n",
      "|    └─Upsample: 2-10                    [-1, 128, 256, 256]       --\n",
      "|    └─double_conv: 2-11                 [-1, 64, 256, 256]        --\n",
      "|    |    └─Sequential: 3-12             [-1, 64, 256, 256]        184,704\n",
      "├─up: 1-9                                [-1, 64, 512, 512]        --\n",
      "|    └─Upsample: 2-12                    [-1, 64, 512, 512]        --\n",
      "|    └─double_conv: 2-13                 [-1, 64, 512, 512]        --\n",
      "|    |    └─Sequential: 3-13             [-1, 64, 512, 512]        110,976\n",
      "├─outconv: 1-10                          [-1, 4, 512, 512]         --\n",
      "|    └─Conv2d: 2-14                      [-1, 4, 512, 512]         260\n",
      "==========================================================================================\n",
      "Total params: 13,394,372\n",
      "Trainable params: 13,394,372\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 75.16\n",
      "==========================================================================================\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 1256.00\n",
      "Params size (MB): 51.10\n",
      "Estimated Total Size (MB): 1308.10\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph.portugal/.local/lib/python3.9/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─inconv: 1-1                            [-1, 64, 512, 512]        --\n",
       "|    └─double_conv: 2-1                  [-1, 64, 512, 512]        --\n",
       "|    |    └─Sequential: 3-1              [-1, 64, 512, 512]        37,824\n",
       "├─down: 1-2                              [-1, 128, 256, 256]       --\n",
       "|    └─Sequential: 2-2                   [-1, 128, 256, 256]       --\n",
       "|    |    └─MaxPool2d: 3-2               [-1, 64, 256, 256]        --\n",
       "|    |    └─double_conv: 3-3             [-1, 128, 256, 256]       221,952\n",
       "├─down: 1-3                              [-1, 256, 128, 128]       --\n",
       "|    └─Sequential: 2-3                   [-1, 256, 128, 128]       --\n",
       "|    |    └─MaxPool2d: 3-4               [-1, 128, 128, 128]       --\n",
       "|    |    └─double_conv: 3-5             [-1, 256, 128, 128]       886,272\n",
       "├─down: 1-4                              [-1, 512, 64, 64]         --\n",
       "|    └─Sequential: 2-4                   [-1, 512, 64, 64]         --\n",
       "|    |    └─MaxPool2d: 3-6               [-1, 256, 64, 64]         --\n",
       "|    |    └─double_conv: 3-7             [-1, 512, 64, 64]         3,542,016\n",
       "├─down: 1-5                              [-1, 512, 32, 32]         --\n",
       "|    └─Sequential: 2-5                   [-1, 512, 32, 32]         --\n",
       "|    |    └─MaxPool2d: 3-8               [-1, 512, 32, 32]         --\n",
       "|    |    └─double_conv: 3-9             [-1, 512, 32, 32]         4,721,664\n",
       "├─up: 1-6                                [-1, 256, 64, 64]         --\n",
       "|    └─Upsample: 2-6                     [-1, 512, 64, 64]         --\n",
       "|    └─double_conv: 2-7                  [-1, 256, 64, 64]         --\n",
       "|    |    └─Sequential: 3-10             [-1, 256, 64, 64]         2,950,656\n",
       "├─up: 1-7                                [-1, 128, 128, 128]       --\n",
       "|    └─Upsample: 2-8                     [-1, 256, 128, 128]       --\n",
       "|    └─double_conv: 2-9                  [-1, 128, 128, 128]       --\n",
       "|    |    └─Sequential: 3-11             [-1, 128, 128, 128]       738,048\n",
       "├─up: 1-8                                [-1, 64, 256, 256]        --\n",
       "|    └─Upsample: 2-10                    [-1, 128, 256, 256]       --\n",
       "|    └─double_conv: 2-11                 [-1, 64, 256, 256]        --\n",
       "|    |    └─Sequential: 3-12             [-1, 64, 256, 256]        184,704\n",
       "├─up: 1-9                                [-1, 64, 512, 512]        --\n",
       "|    └─Upsample: 2-12                    [-1, 64, 512, 512]        --\n",
       "|    └─double_conv: 2-13                 [-1, 64, 512, 512]        --\n",
       "|    |    └─Sequential: 3-13             [-1, 64, 512, 512]        110,976\n",
       "├─outconv: 1-10                          [-1, 4, 512, 512]         --\n",
       "|    └─Conv2d: 2-14                      [-1, 4, 512, 512]         260\n",
       "==========================================================================================\n",
       "Total params: 13,394,372\n",
       "Trainable params: 13,394,372\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 75.16\n",
       "==========================================================================================\n",
       "Input size (MB): 1.00\n",
       "Forward/backward pass size (MB): 1256.00\n",
       "Params size (MB): 51.10\n",
       "Estimated Total Size (MB): 1308.10\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else torch.nn.CrossEntropyLoss()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "#Creating the model\n",
    "print(\"Creating model...\")\n",
    "if name == 'NestedUNet':\n",
    "    model = NestedUNet(num_classes=4)\n",
    "else:\n",
    "    model = UNet(n_channels=1, n_classes=4)\n",
    "model = model.cuda() if torch.cuda.is_available() else model\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We can use \", torch.cuda.device_count(), \" GPUs.\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "if optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "elif optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(params, lr=lr, momentum=momentum, nesterov=nesterov, weight_decay=weight_decay)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "summary.summary(model,(1,512,512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxuoE_jLfngi"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1665649187965,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "2k5hV9l5fngj",
    "outputId": "56644eb8-0380-4b09-cda2-959be3278ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Original images: 980, masks: 980 for training.\n",
      "Original images: 177, masks: 177 for validation.\n",
      "Ratio between Validation and Training is 0.180612\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "#directory of Images and Masks folders (generated from preprocessing)                                         \n",
    "IMAGE_DIR = '/scratch1/joseph.portugal/LIDC-IDRI Preprocessed Exp 3/Image/'\n",
    "MASK_DIR = '/scratch1/joseph.portugal/LIDC-IDRI Preprocessed Exp 3/Mask/'                                                                \n",
    "\n",
    "#meta information\n",
    "meta = pd.read_csv('/scratch1/joseph.portugal/LIDC-IDRI Preprocessed Exp 3/Meta/meta.csv')\n",
    "meta = meta[meta['patient_diagnosis'] != 0]\n",
    "\n",
    "#Get train/test label from metadata file\n",
    "meta['original_image'] = meta['original_image'].apply(lambda x: IMAGE_DIR + \"LIDC-IDRI-\" + x[:4] + \"/\" + x + \".npy\")\n",
    "meta['mask_image'] = meta['mask_image'].apply(lambda x: MASK_DIR + \"LIDC-IDRI-\" + x[:4] + \"/\" + x + \".npy\")\n",
    "\n",
    "\n",
    "#Split into training and validation\n",
    "train_meta = meta[meta['data_split']=='Train']\n",
    "val_meta = meta[meta['data_split']=='Validation']\n",
    "\n",
    "#Get training images into list\n",
    "train_image_paths = list(train_meta['original_image'])\n",
    "train_mask_paths = list(train_meta['mask_image'])\n",
    "\n",
    "#Get validation images into list\n",
    "val_image_paths = list(val_meta['original_image'])\n",
    "val_mask_paths = list(val_meta['mask_image'])\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"Original images: {}, masks: {} for training.\".format(len(train_image_paths),len(train_mask_paths)))\n",
    "print(\"Original images: {}, masks: {} for validation.\".format(len(val_image_paths),len(val_mask_paths)))\n",
    "print(\"Ratio between Validation and Training is {:2f}\".format(len(val_image_paths)/len(train_image_paths)))\n",
    "print(\"*\"*50)\n",
    "\n",
    "\n",
    "#Creating custom LIDC dataset\n",
    "train_dataset = LidcDataset(train_image_paths, train_mask_paths, transforms=transform)\n",
    "val_dataset = LidcDataset(val_image_paths, val_mask_paths, transforms=transform)\n",
    "\n",
    "#Creating Dataloader\n",
    "train_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True,\n",
    "  pin_memory=True,\n",
    "  drop_last=True,\n",
    "  num_workers=num_workers\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "  val_dataset,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False,\n",
    "  pin_memory=True,\n",
    "  drop_last=False,\n",
    "  num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFUeTMtlfngj"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1665649187965,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "FMvFNYA-f6V8"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17676140,
     "status": "ok",
     "timestamp": 1665666864090,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "dkIDHbbffngk",
    "outputId": "64e3cafd-ad2d-4898-ddda-0f36cc4935b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [03:41<00:00,  2.73s/it, loss=1.33, iou=0.777, dice=7.64e-12, sensitivity=1]\n",
      "100%|██████████| 15/15 [00:16<00:00,  1.13s/it, val_loss=1.29, val_iou=0.897, val_dice=1.39e-11, val_sensitivity=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100,  Training BCE Loss: 1.3250,  Training DICE: 0.0000,  Training IOU: 0.7774,  Training Sensitivity: 1.0000,  Validation BCE Loss: 1.2853,  Validation DICE: 0.0000,  Validation IOU: 0.8966,  Validation Sensitivity: 1.0000\n",
      "Saved new best model based on DICE metric!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [03:23<00:00,  2.51s/it, loss=1.25, iou=0.922, dice=1.91e-11, sensitivity=1]\n",
      "100%|██████████| 15/15 [00:14<00:00,  1.06it/s, val_loss=1.23, val_iou=0.961, val_dice=5.29e-11, val_sensitivity=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/100,  Training BCE Loss: 1.2536,  Training DICE: 0.0000,  Training IOU: 0.9222,  Training Sensitivity: 1.0000,  Validation BCE Loss: 1.2323,  Validation DICE: 0.0000,  Validation IOU: 0.9614,  Validation Sensitivity: 1.0000\n",
      "Saved new best model based on DICE metric!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [03:26<00:00,  2.55s/it, loss=1.21, iou=0.978, dice=8.02e-11, sensitivity=1]\n",
      "100%|██████████| 15/15 [00:13<00:00,  1.09it/s, val_loss=1.2, val_iou=0.993, val_dice=1.36e-9, val_sensitivity=1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/100,  Training BCE Loss: 1.2140,  Training DICE: 0.0000,  Training IOU: 0.9776,  Training Sensitivity: 1.0000,  Validation BCE Loss: 1.2037,  Validation DICE: 0.0000,  Validation IOU: 0.9926,  Validation Sensitivity: 1.0000\n",
      "Saved new best model based on DICE metric!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [03:26<00:00,  2.54s/it, loss=1.19, iou=0.994, dice=2.64e-10, sensitivity=1]\n",
      "100%|██████████| 15/15 [00:14<00:00,  1.03it/s, val_loss=1.2, val_iou=0.991, val_dice=2.11e-8, val_sensitivity=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/100,  Training BCE Loss: 1.1947,  Training DICE: 0.0000,  Training IOU: 0.9938,  Training Sensitivity: 1.0000,  Validation BCE Loss: 1.1970,  Validation DICE: 0.0000,  Validation IOU: 0.9906,  Validation Sensitivity: 1.0000\n",
      "Saved new best model based on DICE metric!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 27/81 [01:08<02:18,  2.56s/it, loss=1.19, iou=0.996, dice=3.7e-10, sensitivity=1] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12269/1834282283.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#Update average metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mavg_meters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mavg_meters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iou'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mavg_meters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log = pd.DataFrame(index=[], columns=['epoch','lr','loss','iou','dice','sensitivity','val_loss','val_iou'])\n",
    "\n",
    "best_dice = 0\n",
    "trigger = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #Model Training\n",
    "    avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter(), 'dice': AverageMeter(), 'sensitivity': AverageMeter()}\n",
    "    model.train()\n",
    "    pbar = tqdm(total=len(train_loader)) #progress bar\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        input = data[0].cuda()\n",
    "        target = data[1].cuda()\n",
    "        output = model(input)\n",
    "\n",
    "        #Get loss and metric\n",
    "        loss = criterion(output, torch.argmax(target, dim=1))\n",
    "        iou = iou_score_multiclass(output, target, n_classes=4)\n",
    "        dice = dice_coef_multiclass(output, target, n_classes=4)\n",
    "        sensitivity = sensitivity_metric_multiclass(output, target)\n",
    "\n",
    "        #Calculate the gradient and perform optimizing step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Update average metrics\n",
    "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "        avg_meters['iou'].update(iou, input.size(0))\n",
    "        avg_meters['dice'].update(dice, input.size(0))\n",
    "        avg_meters['sensitivity'].update(sensitivity, input.size(0))\n",
    "\n",
    "        postfix = OrderedDict([\n",
    "            ('loss', avg_meters['loss'].avg),\n",
    "            ('iou', avg_meters['iou'].avg),\n",
    "            ('dice', avg_meters['dice'].avg),\n",
    "            ('sensitivity', avg_meters['sensitivity'].avg)\n",
    "        ])\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    train_log = OrderedDict([\n",
    "        ('loss', avg_meters['loss'].avg),\n",
    "        ('iou', avg_meters['iou'].avg),\n",
    "        ('dice', avg_meters['dice'].avg),\n",
    "        ('sensitivity', avg_meters['sensitivity'].avg)\n",
    "    ])\n",
    "\n",
    "\n",
    "    #Model Validation\n",
    "    val_avg_meters = {'val_loss': AverageMeter(), 'val_iou': AverageMeter(), 'val_dice': AverageMeter(), 'val_sensitivity': AverageMeter()}\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(total=len(val_loader))\n",
    "        for i, val_data in enumerate(val_loader):\n",
    "\n",
    "            val_input = val_data[0].cuda()\n",
    "            val_target = val_data[1].cuda()\n",
    "            val_output = model(val_input)\n",
    "\n",
    "            val_loss = criterion(val_output, torch.argmax(val_target, dim=1))\n",
    "            val_iou = iou_score_multiclass(val_output, val_target, n_classes=4)\n",
    "            val_dice = dice_coef_multiclass(val_output, val_target, n_classes=4)\n",
    "            val_sensitivity = sensitivity_metric_multiclass(val_output, val_target)\n",
    "\n",
    "            val_avg_meters['val_loss'].update(val_loss.item(), val_input.size(0))\n",
    "            val_avg_meters['val_iou'].update(val_iou, val_input.size(0))\n",
    "            val_avg_meters['val_dice'].update(val_dice, val_input.size(0))\n",
    "            val_avg_meters['val_sensitivity'].update(val_sensitivity, val_input.size(0))\n",
    "\n",
    "            val_postfix = OrderedDict([\n",
    "                ('val_loss', val_avg_meters['val_loss'].avg),\n",
    "                ('val_iou', val_avg_meters['val_iou'].avg),\n",
    "                ('val_dice', val_avg_meters['val_dice'].avg),\n",
    "                ('val_sensitivity', val_avg_meters['val_sensitivity'].avg)\n",
    "            ])\n",
    "            val_pbar.set_postfix(val_postfix)\n",
    "            val_pbar.update(1)\n",
    "        val_pbar.close()\n",
    "\n",
    "    val_log = OrderedDict([\n",
    "        ('val_loss', val_avg_meters['val_loss'].avg),\n",
    "        ('val_iou', val_avg_meters['val_iou'].avg),\n",
    "        ('val_dice', val_avg_meters['val_dice'].avg),\n",
    "        ('val_sensitivity', val_avg_meters['val_sensitivity'].avg)\n",
    "    ])\n",
    "    \n",
    "\n",
    "    print('Training Epoch {}/{},  Training BCE Loss: {:.4f},  Training DICE: {:.4f},  Training IOU: {:.4f},  Training Sensitivity: {:.4f},  Validation BCE Loss: {:.4f},  Validation DICE: {:.4f},  Validation IOU: {:.4f},  Validation Sensitivity: {:.4f}'.format(\n",
    "        epoch+1, epochs, train_log['loss'], train_log['dice'], train_log['iou'], train_log['sensitivity'], val_log['val_loss'], val_log['val_dice'], val_log['val_iou'], val_log['val_sensitivity']\n",
    "    ))\n",
    "\n",
    "    #Save values to csv file\n",
    "    tmp = pd.Series([\n",
    "        epoch,\n",
    "        lr,\n",
    "        train_log['loss'],\n",
    "        train_log['iou'],\n",
    "        train_log['dice'],\n",
    "        val_log['val_loss'],\n",
    "        val_log['val_iou'],\n",
    "        val_log['val_dice'],\n",
    "        val_log['val_sensitivity']\n",
    "    ], index=['epoch', 'lr', 'loss', 'iou', 'dice', 'val_loss', 'val_iou', 'val_dice', 'val_sensitivity'])\n",
    "\n",
    "    log = log.append(tmp, ignore_index=True)\n",
    "    log.to_csv('model_outputs/{}/log.csv'.format(file_name), index=False)\n",
    "\n",
    "    trigger += 1\n",
    "\n",
    "    #If best DICE score, save the model\n",
    "    if val_log['val_dice'] > best_dice:\n",
    "        torch.save(model.state_dict(), 'model_outputs/{}/model.pth'.format(file_name))\n",
    "        best_dice = val_log['val_dice']\n",
    "        print(\"Saved new best model based on DICE metric!\")\n",
    "        trigger = 0\n",
    "    \n",
    "    if early_stopping >= 0 and trigger >= early_stopping:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1665666864091,
     "user": {
      "displayName": "Lenard Ryan Llarenas",
      "userId": "09745882041434470877"
     },
     "user_tz": -480
    },
    "id": "fPuk2P8ixSn1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1NMVPx_UbhEFBqr9_1Q43e1ttX-lUq_Q1",
     "timestamp": 1665470126399
    }
   ]
  },
  "interpreter": {
   "hash": "cb56554e22e7015c8034fdf988622508a740e712e9b2a73e8d246c741229446d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
