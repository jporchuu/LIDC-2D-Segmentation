{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIDC IDRI 2D SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import transforms\n",
    "import torchsummary as summary\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Unet.unet_model import UNet\n",
    "from UnetNested.Nested_Unet import NestedUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"UNet\"           #default = \"UNet\"; can be NestedUNet\n",
    "epochs = 100            #default = 400\n",
    "batch_size = 12         #default = 12\n",
    "early_stopping = 50     #default = 50\n",
    "num_workers = 6         #default = 8\n",
    "optimizer = 'Adam'      #default = 'Adam'; can be SGD\n",
    "lr = 1e-5               #default = 1e-5\n",
    "momentum = 0.9          #default = 0.9\n",
    "weight_decay = 1e-4     #default = 1e-4\n",
    "nesterov = False        #default = False\n",
    "augmentation = True     #default = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    #Let x be the input and y be the target\n",
    "    def forward(self, x, y):\n",
    "        bce = F.binary_cross_entropy_with_logits(x, y)\n",
    "        smooth = 1e-5\n",
    "        x = torch.sigmoid(x)\n",
    "        num = y.size(0)\n",
    "        x = x.view(num, -1)\n",
    "        y = y.view(num, -1)\n",
    "        intersection = (x*y)\n",
    "        dice = (2. * intersection.sum(1) + smooth) / (x.sum(1) + y.sum(1) + smooth)\n",
    "        dice = 1 - dice.sum() / num\n",
    "\n",
    "        return 0.5 * bce + dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LidcDataset(Dataset):\n",
    "    def __init__(self, IMAGES_PATHS, MASK_PATHS, transforms):\n",
    "        self.image_paths = IMAGES_PATHS\n",
    "        self.mask_paths = MASK_PATHS\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = np.load(self.image_paths[index])\n",
    "        mask = np.load(self.mask_paths[index])\n",
    "\n",
    "        #Make image and mask 3 dimensional\n",
    "        image = image.reshape(512,512,1)\n",
    "        mask = mask.reshape(512,512,1)\n",
    "\n",
    "        #Convert datatype\n",
    "        mask = mask.astype('uint8')\n",
    "\n",
    "        #Apply augmentation\n",
    "        augmented = self.transforms(image=image,mask=mask)\n",
    "        image = augmented['image']\n",
    "        mask = augmented['mask']\n",
    "        mask = mask.reshape([1,512,512])\n",
    "\n",
    "        image, mask = image.type(torch.FloatTensor), mask.type(torch.FloatTensor)     \n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "            A.ElasticTransform(alpha=1.1,alpha_affine=0.5,sigma=5,p=0.15),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_score(output, target):\n",
    "    smooth = 1e-5\n",
    "    if torch.is_tensor(output):\n",
    "        output = torch.sigmoid(output).data.cpu().numpy()\n",
    "    if torch.is_tensor(target):\n",
    "        target = target.data.cpu().numpy()\n",
    "    output_ = output > 0.5\n",
    "    target_ = target > 0.5\n",
    "    intersection = (output_ & target_).sum()\n",
    "    union = (output_ | target_).sum()\n",
    "\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "\n",
    "def dice_coef(output, target):\n",
    "    smooth = 1e-5\n",
    "    #Sigmoid is used because the U-Net output is logit\n",
    "    output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n",
    "    target = target.view(-1).data.cpu().numpy()\n",
    "    intersection = (output*target).sum()\n",
    "\n",
    "    return (2. * intersection + smooth) / \\\n",
    "        (output.sum() + target.sum() + smooth)\n",
    "\n",
    "\n",
    "def sensitivity_metric(output, target):\n",
    "    eps = 1e-5\n",
    "    output = torch.sigmoid(output).view(-1).data.cpu()\n",
    "    target = target.view(-1).data.cpu()\n",
    "    # elements of confusion matrix\n",
    "    tp = torch.sum(output * target) # True Positive\n",
    "    fp = torch.sum(output * (1 - target)) # False Positive\n",
    "    fn = torch.sum((1 - output) * target) # False Negative\n",
    "    tn = torch.sum((1 - output) * (1 - target)) # True Negative\n",
    "    # compute sensitivity\n",
    "    sensitivity = (tp + eps) / (tp + fn + eps)\n",
    "    \n",
    "    return sensitivity.item()\n",
    "\n",
    "\n",
    "def dice_coef2(output, target):\n",
    "    \"This metric is for validation\"\n",
    "    smooth = 1e-5\n",
    "    output = output.view(-1)\n",
    "    output = (output>0.5).float().cpu().numpy()\n",
    "    target = target.view(-1).data.cpu().numpy()\n",
    "    intersection = (output*target).sum()\n",
    "\n",
    "    return (2. * intersection + smooth) / \\\n",
    "        (output.sum() + target.sum() + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(v):\n",
    "    if v.lower() in ['true', 1]:\n",
    "        return True\n",
    "    elif v.lower() in ['false', 0]:\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    #Computes and stores the average and current value\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if augmentation == True:\n",
    "    file_name = name + '_with_augmentation'\n",
    "else:\n",
    "    file_name = name + '_base'\n",
    "os.makedirs('model_outputs/{}'.format(file_name), exist_ok=True)\n",
    "print(\"Creating directory called \", file_name)\n",
    "\n",
    "print('-' * 20)\n",
    "print(\"Configuration Setting: \")\n",
    "print(\"Model: \", name)\n",
    "print(\"Max Epochs: \", epochs)\n",
    "print(\"Batch Size: \", batch_size)\n",
    "print(\"Number of Workers: \", num_workers)\n",
    "print(\"Optimizer: \", optimizer)\n",
    "print(\"Learning Rate: \", lr)\n",
    "print(\"Augmentation: \", augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BCEDiceLoss().cuda()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "#Creating the model\n",
    "print(\"Creating model...\")\n",
    "if name == 'NestedUNet':\n",
    "    model = NestedUNet(num_classes=1)\n",
    "else:\n",
    "    model = UNet(n_channels=1, n_classes=1, bilinear=True)\n",
    "model = model.cuda()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"We can use \", torch.cuda.device_count(), \" GPUs.\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "if optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "elif optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(params, lr=lr, momentum=momentum, nesterov=nesterov, weight_decay=weight_decay)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "summary.summary(model,(1,512,512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory of Images and Masks folders (generated from preprocessing)                                         \n",
    "IMAGE_DIR = '/scratch1/joseph.portugal/LIDC-IDRI Preprocessed/Image/'\n",
    "MASK_DIR = '/scratch1/joseph.portugal/LIDC-IDRI Preprocessed/Mask/'                                                                 \n",
    "\n",
    "#meta information\n",
    "meta = pd.read_csv('/scratch1/joseph.portugal/LIDC-IDRI Preprocessed/Meta/meta.csv')     \n",
    "\n",
    "#Get train/test label from metadata file\n",
    "meta['original_image'] = meta['original_image'].apply(lambda x: IMAGE_DIR + \"LIDC-IDRI-\" + x[:4] + \"/\" + x + \".npy\")\n",
    "meta['mask_image'] = meta['mask_image'].apply(lambda x: MASK_DIR + \"LIDC-IDRI-\" + x[:4] + \"/\" + x + \".npy\")\n",
    "\n",
    "\n",
    "#Split into training and validation\n",
    "train_meta = meta[meta['data_split']=='Train']\n",
    "val_meta = meta[meta['data_split']=='Validation']\n",
    "\n",
    "#Get training images into list\n",
    "train_image_paths = list(train_meta['original_image'])\n",
    "train_mask_paths = list(train_meta['mask_image'])\n",
    "\n",
    "#Get validation images into list\n",
    "val_image_paths = list(val_meta['original_image'])\n",
    "val_mask_paths = list(val_meta['mask_image'])\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"Original images: {}, masks: {} for training.\".format(len(train_image_paths),len(train_mask_paths)))\n",
    "print(\"Original images: {}, masks: {} for validation.\".format(len(val_image_paths),len(val_mask_paths)))\n",
    "print(\"Ratio between Validation and Training is {:2f}\".format(len(val_image_paths)/len(train_image_paths)))\n",
    "print(\"*\"*50)\n",
    "\n",
    "\n",
    "#Creating custom LIDC dataset\n",
    "train_dataset = LidcDataset(train_image_paths, train_mask_paths, transforms=transform)\n",
    "val_dataset = LidcDataset(val_image_paths, val_mask_paths, transforms=transform)\n",
    "\n",
    "#Creating Dataloader\n",
    "train_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True,\n",
    "  pin_memory=True,\n",
    "  drop_last=True,\n",
    "  num_workers=num_workers\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "  val_dataset,\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False,\n",
    "  pin_memory=True,\n",
    "  drop_last=False,\n",
    "  num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.DataFrame(index=[], columns=['epoch','lr','loss','iou','dice','sensitivity','val_loss','val_iou'])\n",
    "\n",
    "best_dice = 0\n",
    "trigger = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #Model Training\n",
    "    avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter(), 'dice': AverageMeter(), 'sensitivity': AverageMeter()}\n",
    "    model.train()\n",
    "    pbar = tqdm(total=len(train_loader)) #progress bar\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        input = data[0].cuda()\n",
    "        target = data[1].cuda()\n",
    "        output = model(input)\n",
    "\n",
    "        #Get loss and metric\n",
    "        loss = criterion(output, target)\n",
    "        iou = iou_score(output, target)\n",
    "        dice = dice_coef(output, target)\n",
    "        sensitivity = sensitivity_metric(output, target)\n",
    "\n",
    "        #Calculate the gradient and perform optimizing step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Update average metrics\n",
    "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "        avg_meters['iou'].update(iou, input.size(0))\n",
    "        avg_meters['dice'].update(dice, input.size(0))\n",
    "        avg_meters['sensitivity'].update(sensitivity, input.size(0))\n",
    "\n",
    "        postfix = OrderedDict([\n",
    "            ('loss', avg_meters['loss'].avg),\n",
    "            ('iou', avg_meters['iou'].avg),\n",
    "            ('dice', avg_meters['dice'].avg),\n",
    "            ('sensitivity', avg_meters['sensitivity'].avg)\n",
    "        ])\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    train_log = OrderedDict([\n",
    "        ('loss', avg_meters['loss'].avg),\n",
    "        ('iou', avg_meters['iou'].avg),\n",
    "        ('dice', avg_meters['dice'].avg),\n",
    "        ('sensitivity', avg_meters['sensitivity'].avg)\n",
    "    ])\n",
    "\n",
    "\n",
    "    #Model Validation\n",
    "    val_avg_meters = {'val_loss': AverageMeter(), 'val_iou': AverageMeter(), 'val_dice': AverageMeter(), 'val_sensitivity': AverageMeter()}\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(total=len(val_loader))\n",
    "        for i, val_data in enumerate(val_loader):\n",
    "\n",
    "            val_input = val_data[0].cuda()\n",
    "            val_target = val_data[1].cuda()\n",
    "            val_output = model(val_input)\n",
    "\n",
    "            val_loss = criterion(val_output, val_target)\n",
    "            val_iou = iou_score(val_output, val_target)\n",
    "            val_dice = dice_coef(val_output, val_target)\n",
    "            val_sensitivity = sensitivity_metric(val_output, val_target)\n",
    "\n",
    "            val_avg_meters['val_loss'].update(val_loss.item(), val_input.size(0))\n",
    "            val_avg_meters['val_iou'].update(val_iou, val_input.size(0))\n",
    "            val_avg_meters['val_dice'].update(val_dice, val_input.size(0))\n",
    "            val_avg_meters['val_sensitivity'].update(val_sensitivity, val_input.size(0))\n",
    "\n",
    "            val_postfix = OrderedDict([\n",
    "                ('val_loss', val_avg_meters['val_loss'].avg),\n",
    "                ('val_iou', val_avg_meters['val_iou'].avg),\n",
    "                ('val_dice', val_avg_meters['val_dice'].avg),\n",
    "                ('val_sensitivity', val_avg_meters['val_sensitivity'].avg)\n",
    "            ])\n",
    "            val_pbar.set_postfix(val_postfix)\n",
    "            val_pbar.update(1)\n",
    "        val_pbar.close()\n",
    "\n",
    "    val_log = OrderedDict([\n",
    "        ('val_loss', val_avg_meters['val_loss'].avg),\n",
    "        ('val_iou', val_avg_meters['val_iou'].avg),\n",
    "        ('val_dice', val_avg_meters['val_dice'].avg),\n",
    "        ('val_sensitivity', val_avg_meters['val_sensitivity'].avg)\n",
    "    ])\n",
    "    \n",
    "\n",
    "    print('Training Epoch {}/{},  Training BCE Loss: {:.4f},  Training DICE: {:.4f},  Training IOU: {:.4f},  Training Sensitivity: {:.4f},  Validation BCE Loss: {:.4f},  Validation DICE: {:.4f},  Validation IOU: {:.4f},  Validation Sensitivity: {:.4f}'.format(\n",
    "        epoch+1, epochs, train_log['loss'], train_log['dice'], train_log['iou'], train_log['sensitivity'], val_log['val_loss'], val_log['val_dice'], val_log['val_iou'], val_log['val_sensitivity']\n",
    "    ))\n",
    "\n",
    "    #Save values to csv file\n",
    "    tmp = pd.Series([\n",
    "        epoch,\n",
    "        lr,\n",
    "        train_log['loss'],\n",
    "        train_log['iou'],\n",
    "        train_log['dice'],\n",
    "        val_log['val_loss'],\n",
    "        val_log['val_iou'],\n",
    "        val_log['val_dice'],\n",
    "        val_log['val_sensitivity']\n",
    "    ], index=['epoch', 'lr', 'loss', 'iou', 'dice', 'val_loss', 'val_iou', 'val_dice', 'val_sensitivity'])\n",
    "\n",
    "    log = log.append(tmp, ignore_index=True)\n",
    "    log.to_csv('model_outputs/{}/log.csv'.format(file_name), index=False)\n",
    "\n",
    "    trigger += 1\n",
    "\n",
    "    #If best DICE score, save the model\n",
    "    if val_log['val_dice'] > best_dice:\n",
    "        torch.save(model.state_dict(), 'model_outputs/{}/model.pth'.format(file_name))\n",
    "        best_dice = val_log['val_dice']\n",
    "        print(\"Saved new best model based on DICE metric!\")\n",
    "        trigger = 0\n",
    "    \n",
    "    if early_stopping >= 0 and trigger >= early_stopping:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb56554e22e7015c8034fdf988622508a740e712e9b2a73e8d246c741229446d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
